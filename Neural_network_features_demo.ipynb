{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28200af2-c3f4-4fc4-b6bd-f18139bcaad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a81e088-5153-48a2-93d5-d794eb7d1689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SAS dataset\n",
    "file_path = 'C:/Users/khtur/Desktop/Eas/Models/abt_app.sas7bdat'  # Replace with your file path\n",
    "data = pd.read_sas(file_path)\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(columns='default12')  # Replace 'default12' with your actual target column\n",
    "y = data['default12']\n",
    "\n",
    "# Drop rows where the target variable is NaN\n",
    "X_cleaned = X[~y.isna()]\n",
    "y_cleaned = y[~y.isna()]\n",
    "\n",
    "# Encode the target to ensure it's binary (0 and 1)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_cleaned)  # Convert target values to 0 and 1.\n",
    "\n",
    "# Check class imbalance after encoding\n",
    "print(np.bincount(y_encoded))  # Display counts of 0s and 1s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec0af4e-d70d-4a9e-a158-17583d29ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Step 1: Separate numeric and categorical columns\n",
    "numeric_cols = X_cleaned.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = X_cleaned.select_dtypes(include=[object]).columns.tolist()\n",
    "\n",
    "# Step 2: Define the preprocessing pipeline with imputation\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values for numeric features\n",
    "            ('scaler', StandardScaler())]  # Scale numeric features\n",
    "        ), numeric_cols),\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing values for categorical features\n",
    "            ('encoder', OneHotEncoder(drop='first'))]  # One-hot encode categorical features\n",
    "        ), categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Step 3: Apply the transformations\n",
    "X_transformed = preprocessor.fit_transform(X_cleaned)\n",
    "\n",
    "# Convert to dense matrix (if necessary)\n",
    "if isinstance(X_transformed, np.ndarray) is False:\n",
    "    X_transformed = X_transformed.toarray()  # Convert sparse matrix to dense\n",
    "\n",
    "# Step 4: Split data into train and validation sets\n",
    "X_train_scaled, X_val_scaled, y_train, y_val = train_test_split(X_transformed, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print shapes of transformed data\n",
    "print(f\"Train data shape: {X_train_scaled.shape}\")\n",
    "print(f\"Validation data shape: {X_val_scaled.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee8e165-17b0-4d80-b827-62e5b377a10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Define the number of principal components to retain (e.g., 100 components)\n",
    "pca = PCA(n_components=100)\n",
    "\n",
    "# Fit PCA on the training data and transform both train and validation sets\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_val_pca = pca.transform(X_val_scaled)\n",
    "\n",
    "# Check the explained variance to ensure we're capturing enough information\n",
    "explained_variance = pca.explained_variance_ratio_.sum()\n",
    "print(f\"Total explained variance by 100 components: {explained_variance:.2f}\")\n",
    "\n",
    "# Print the new shape of the transformed dataset\n",
    "print(f\"Shape of PCA-transformed train data: {X_train_pca.shape}\")\n",
    "print(f\"Shape of PCA-transformed validation data: {X_val_pca.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d935b0fa-7d9e-42c3-b6f0-f96b23eafed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def build_and_train_model(X_train, y_train, X_val, y_val):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1],)),  # Input size is now the reduced number of components\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.01)),  # Added L2 regularization\n",
    "        Dropout(0.4),  # Increased dropout to help prevent overfitting\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.4),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)\n",
    "    \n",
    "    # Calculate class weights based on encoded labels\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = {int(cls): weight for cls, weight in zip(np.unique(y_train), class_weights)}\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), \n",
    "                        epochs=100, batch_size=32, callbacks=[early_stopping], \n",
    "                        class_weight=class_weight_dict, verbose=1)\n",
    "    \n",
    "    print(f\"Training stopped at epoch {len(history.epoch)} due to early stopping\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Build and train the model using PCA-transformed data\n",
    "model, history = build_and_train_model(X_train_pca, y_train, X_val_pca, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5e0f2b-d354-4c63-bc45-1e425d771c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for validation set\n",
    "y_val_prob = model.predict(X_val_pca).ravel()\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_val, y_val_prob)\n",
    "\n",
    "# Calculate AUC\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e099d44-8fee-400b-80d7-28ba43ad9515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Train Logistic Regression on PCA-transformed data\n",
    "log_model = LogisticRegression(max_iter=1000)\n",
    "log_model.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predict probabilities for validation set\n",
    "y_val_prob_log = log_model.predict_proba(X_val_pca)[:, 1]\n",
    "\n",
    "# Calculate AUC for Logistic Regression\n",
    "log_auc = roc_auc_score(y_val, y_val_prob_log)\n",
    "print(f'Logistic Regression AUC: {log_auc:.2f}')\n",
    "\n",
    "# Plot ROC curve for Logistic Regression\n",
    "fpr_log, tpr_log, _ = roc_curve(y_val, y_val_prob_log)\n",
    "plt.plot(fpr_log, tpr_log, color='green', label=f'Logistic Regression ROC (AUC = {log_auc:.2f})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Logistic Regression')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f6f8b0-3778-49fb-bd7e-84cb82e4dd27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
